import os
from tools.logger import setup_logging
from tools.config import get_max_tokens, LLM_MODEL_SETTINGS, LLM_OUTPUT_SETTINGS
from tools.text_processing import fix_hebrew_grammar_errors, convert_tagged_text_to_html
from tools.entity_analyzer import analyze_text_for_llm_with_cache
import re
import requests
import json

logger = setup_logging()

HEBREW_VOCAB_PATH = os.path.join(os.path.dirname(__file__), '../data/hebrew_vocabulary.json')

def load_hebrew_vocabulary(context_text=None, max_terms=25, max_length=1000):
    """Load only relevant Hebrew vocabulary terms for prompt enrichment."""
    try:
        with open(HEBREW_VOCAB_PATH, encoding='utf-8') as f:
            vocab = json.load(f)
        financial_terms = vocab.get('financial_terms', {})
        found = []
        if context_text:
            for eng, heb in financial_terms.items():
                if eng in context_text:
                    found.append(f"{eng} â†’ {heb}")
                if len(found) >= max_terms:
                    break
        # If not enough found, add some common terms
        if len(found) < 5:
            for eng, heb in list(financial_terms.items())[:max_terms-len(found)]:
                if f"{eng} â†’ {heb}" not in found:
                    found.append(f"{eng} â†’ {heb}")
        snippet = "\n".join(found)
        if len(snippet) > max_length:
            snippet = snippet[:max_length-3] + "..."
        return snippet
    except Exception as e:
        return ""


def parse_sentiment_blocks(text):
    """
    ××—×œ×§ ×˜×§×¡×˜ ×œ×‘×œ×•×§×™× ×œ×¤×™ ×¡× ×˜×™×× ×˜ (×—×™×•×‘×™, × ×™×™×˜×¨×œ×™, ×©×œ×™×œ×™)
    ××—×–×™×¨ dict: {'positive': [...], 'neutral': [...], 'negative': [...]}
    """
    blocks = {'positive': [], 'neutral': [], 'negative': []}
    current = None
    for line in text.splitlines():
        line = line.strip()
        if line.startswith('Positive Sentiment:'):
            current = 'positive'
            continue
        elif line.startswith('Neutral Sentiment:'):
            current = 'neutral'
            continue
        elif line.startswith('Negative Sentiment:'):
            current = 'negative'
            continue
        elif line.startswith('Posted') or line.startswith('AI Generated') or not line:
            continue
        if current:
            blocks[current].append(line)
    return blocks

def extract_entities(text):
    """
    
    ××–×”×” ×™×©×•×™×•×ª ×‘×× ×’×œ×™×ª (×¤×©×•×˜: ××™×œ×™× ×’×“×•×œ×•×ª/×¡×™×× ×™ ××¡×—×¨/×¡×™××•×œ) ×œ×©×™××•×¨ ×‘×ª×¨×’×•×
    """
    # ×“×•×’××” ×¤×©×•×˜×”: ×›×œ ××™×œ×” ×‘××•×ª×™×•×ª ×’×“×•×œ×•×ª ××• ×¢× × ×§×•×“×”/×¡×•×’×¨×™×™×
    pattern = re.compile(r'\b([A-Z][A-Za-z0-9&.()\-]+)\b')
    return set(pattern.findall(text))

def hebrewize(text, entities=None):
    """
    ×××™×¨ ×˜×§×¡×˜ ×œ×× ×’×œ×™×ª ×‘×©×¤×” ×¢×‘×¨×™×ª (×¤×©×•×˜: ×ª×¨×’×•× ××™×œ×™× ×‘×¡×™×¡×™, ×©××™×¨×” ×¢×œ ×™×©×•×™×•×ª ×‘×× ×’×œ×™×ª)
    """
    if not text:
        return ''
    # ×©××•×¨ ×¢×œ ×™×©×•×™×•×ª ×‘×× ×’×œ×™×ª
    if entities:
        for ent in entities:
            text = re.sub(rf'\b{re.escape(ent)}\b', f'[[{ent}]]', text)
    # ×ª×¨×’×•× ×‘×¡×™×¡×™ (×¤×™×™×§): ×”×—×œ×£ ××™×œ×™× × ×¤×•×¦×•×ª
    replacements = {
        'shares': '×× ×™×”',
        'stock': '×× ×™×”',
        'price': '××—×™×¨',
        'growth': '×¦××™×—×”',
        'decline': '×™×¨×™×“×”',
        'increase': '×¢×œ×™×™×”',
        'decrease': '×™×¨×™×“×”',
        'investor': '××©×§×™×¢',
        'investors': '××©×§×™×¢×™×',
        'company': '×—×‘×¨×”',
        'earnings': '×¨×•×•×—×™×',
        'loss': '×”×¤×¡×“',
        'positive': '×—×™×•×‘×™',
        'negative': '×©×œ×™×œ×™',
        'neutral': '× ×™×™×˜×¨×œ×™',
        'report': '×“×•×—',
        'target': '×™×¢×“',
        'forecast': '×ª×—×–×™×ª',
        'analyst': '×× ×œ×™×¡×˜',
        'analysts': '×× ×œ×™×¡×˜×™×',
        'agreement': '×”×¡×›×',
        'deal': '×¢×¡×§×”',
        'pipeline': '×¦× ×¨×ª',
        'AI': '×‘×™× ×” ××œ××›×•×ª×™×ª',
        'innovation': '×—×“×©× ×•×ª',
        'risk': '×¡×™×›×•×Ÿ',
        'opportunity': '×”×–×“×× ×•×ª',
        'market': '×©×•×§',
        'revenue': '×”×›× ×¡×”',
        'profit': '×¨×•×•×—',
        'losses': '×”×¤×¡×“×™×',
        'buy': '×§× ×™×™×”',
        'sell': '××›×™×¨×”',
        'hold': '×”×—×–×§×”',
        'rating': '×“×™×¨×•×’',
        'outlook': '×ª×—×–×™×ª',
        'trend': '××’××”',
        'bullish': '×©×•×¨×™',
        'bearish': '×“×•×‘×™',
        'dividend': '×“×™×‘×™×“× ×“',
        'partnership': '×©×™×ª×•×£ ×¤×¢×•×œ×”',
        'acquisition': '×¨×›×™×©×”',
        'merger': '××™×–×•×’',
        'lawsuit': '×ª×‘×™×¢×”',
        'legal': '××©×¤×˜×™',
        'court': '×‘×™×ª ××©×¤×˜',
        'CEO': '×× ×›"×œ',
        'CFO': '×¡×× ×›"×œ ×›×¡×¤×™×',
        'Q2': '×¨×‘×¢×•×Ÿ ×©× ×™',
        'Q3': '×¨×‘×¢×•×Ÿ ×©×œ×™×©×™',
        'Q4': '×¨×‘×¢×•×Ÿ ×¨×‘×™×¢×™',
        'Q1': '×¨×‘×¢×•×Ÿ ×¨××©×•×Ÿ',
    }
    for en, he in replacements.items():
        text = re.sub(rf'\b{en}\b', he, text, flags=re.IGNORECASE)
    # ×”×—×–×¨ ××ª ×”×™×©×•×™×•×ª ×”××§×•×¨×™×•×ª
    if entities:
        for ent in entities:
            text = text.replace(f'[[{ent}]]', ent)
    # ×¡×™××•×Ÿ RTL
    text = f'<span dir="rtl">{text}</span>'
    return fix_hebrew_grammar_errors(text)

def build_hebrew_article(title, sentiment_blocks, entities=None, as_html=True):
    """
    ×‘×•× ×” HTML ×©×œ ××××¨ ×‘×¢×‘×¨×™×ª ×¢× ×©×œ×•×©×” ×‘×œ×•×§×™×, ××• ×˜×§×¡×˜ ×‘×œ×‘×“ (×× as_html=False)
    """
    sentiment_titles = {
        'positive': '× ×§×•×“×•×ª ×—×™×•×‘×™×•×ª',
        'neutral': '× ×§×•×“×•×ª × ×™×™×˜×¨×œ×™×•×ª',
        'negative': '× ×§×•×“×•×ª ×©×œ×™×œ×™×•×ª',
    }
    if as_html:
        html = [f'<h1 dir="rtl" style="text-align:right">{title}</h1>']
        for key in ['positive', 'neutral', 'negative']:
            if sentiment_blocks[key]:
                html.append(f'<h2 dir="rtl" style="text-align:right">{sentiment_titles[key]}</h2>')
                html.append('<ul dir="rtl" style="text-align:right">')
                for sent in sentiment_blocks[key]:
                    heb = hebrewize(sent, entities)
                    html.append(f'<li>{heb}</li>')
                html.append('</ul>')
        return '\n'.join(html)
    else:
        # ×˜×§×¡×˜ ×‘×œ×‘×“
        lines = [title]
        for key in ['positive', 'neutral', 'negative']:
            if sentiment_blocks[key]:
                lines.append(f'--- {sentiment_titles[key]} ---')
                for sent in sentiment_blocks[key]:
                    heb = hebrewize(sent, entities)
                    lines.append(f'- {heb}')
        return '\n'.join(lines)

def wrap_full_html(body_html, title="MarketBit ××××¨ ×‘×¢×‘×¨×™×ª"): 
    """
    ×¢×•×˜×£ ××ª ×’×•×£ ×”××××¨ ×‘-html ××œ× ×›×•×œ×œ head, RTL, meta ×•×›×•'.
    """
    return f'''<!DOCTYPE html>
<html lang="he" dir="rtl">
<head>
    <meta charset="utf-8">
    <title>{title}</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>body {{ direction: rtl; text-align: right; font-family: Arial, sans-serif; }}</style>
</head>
<body>
{body_html}
</body>
</html>'''

def process_article_file(filepath, output_html_path=None, ticker=None, as_html=True):
    """
    ×§×•×¨× ×§×•×‘×¥ original, ××¤×™×§ ×‘×œ×•×§×™×, ×××™×¨ ×œ×¢×‘×¨×™×ª, ×‘×•× ×” HTML ××œ× ××• ×˜×§×¡×˜ ×‘×œ×‘×“, ×©×•××¨/××—×–×™×¨ ×¤×œ×˜.
    :param filepath: × ×ª×™×‘ ×œ×§×•×‘×¥ original
    :param output_html_path: × ×ª×™×‘ ×œ×©××™×¨×ª HTML (×œ× ×—×•×‘×”)
    :param ticker: ×¡×™××•×œ (×œ× ×—×•×‘×”)
    :param as_html: ×”×× ×œ×”×—×–×™×¨ HTML ××œ× (True) ××• ×˜×§×¡×˜ ×‘×œ×‘×“ (False)
    :return: ××—×¨×•×–×ª HTML ××• ×˜×§×¡×˜
    """
    if not os.path.exists(filepath):
        logger.error(f"âŒ File not found: {filepath}")
        raise FileNotFoundError(f"File not found: {filepath}")
    with open(filepath, 'r', encoding='utf-8') as f:
        text = f.read()
    if not text.strip():
        logger.error(f"âŒ File is empty: {filepath}")
        raise ValueError(f"File is empty: {filepath}")
    logger.info(f"ğŸ” Processing file: {filepath}")
    # ×”×¤×§×ª ×™×©×•×™×•×ª (××¤×©×¨×™)
    entities_context = None
    if ticker:
        try:
            entities_context = analyze_text_for_llm_with_cache(text, ticker)
        except Exception as e:
            logger.warning(f"Entity analysis failed: {e}")
    # ×›×•×ª×¨×ª ×¨××©×™×ª
    first_line = text.split('\n', 1)[0].strip()
    entities = extract_entities(text)
    title = hebrewize(first_line, entities)
    # ×—×œ×•×§×” ×œ×‘×œ×•×§×™×
    sentiment_blocks = parse_sentiment_blocks(text)
    # ×‘× ×™×™×ª ×’×•×£ ×”××××¨
    body = build_hebrew_article(title, sentiment_blocks, entities, as_html=as_html)
    if as_html:
        html = wrap_full_html(body, title=first_line)
        html = convert_tagged_text_to_html(html)
        if output_html_path:
            with open(output_html_path, 'w', encoding='utf-8') as f:
                f.write(html)
            logger.info(f"âœ… Saved HTML article to {output_html_path}")
        return html
    else:
        if output_html_path:
            with open(output_html_path, 'w', encoding='utf-8') as f:
                f.write(body)
            logger.info(f"âœ… Saved text article to {output_html_path}")
        return body

def format_llm_output(text):
    """Convert markdown emphasis (**text**) to HTML <strong> tags and add line breaks."""
    import re
    # Convert **text** to <strong>text</strong>
    text = re.sub(r'\*\*(.*?)\*\*', r'<strong>\1</strong>', text)
    # Add line breaks for better readability
    text = text.replace('. ', '.<br>')
    text = text.replace('! ', '!<br>')
    text = text.replace('? ', '?<br>')
    return text


def parse_ollama_response(response):
    """Safely parse Ollama response, supporting NDJSON or multiple JSON objects."""
    try:
        # Try standard JSON
        raw_text = response.json().get('response', '')
    except Exception:
        # Try NDJSON (newline-delimited JSON)
        lines = response.text.strip().splitlines()
        for line in lines:
            try:
                obj = json.loads(line)
                if 'response' in obj:
                    raw_text = obj['response']
                    break
            except Exception:
                continue
        else:
            # Fallback: return raw text
            raw_text = response.text
    
    # Format the output
    return format_llm_output(raw_text)

def mark_entities(text, entities):
    """×¡××Ÿ ×™×©×•×™×•×ª ×‘×˜×§×¡×˜ ×‘-[[...]] ×›×“×™ ×©×”××•×“×œ ×œ× ×™×ª×¨×’× ××•×ª×Ÿ"""
    for ent in sorted(entities, key=len, reverse=True):
        text = re.sub(rf'\b{re.escape(ent)}\b', f'[[{ent}]]', text)
    return text

def restore_marked_entities(text, entities):
    """×”×—×–×¨ ×›×œ ×™×©×•×ª ×©×¡×•×× ×” ×‘-[[...]] ×œ××§×•×¨ ×‘×× ×’×œ×™×ª, ×’× ×× ×”××•×“×œ ×©×™× ×” ××•×ª×”"""
    for ent in sorted(entities, key=len, reverse=True):
        # ×ª×—×–×™×¨ ×›×œ ××•×¤×¢ ×©×œ [[...]] ×œ-ent ×”××§×•×¨×™
        text = re.sub(rf'\[\[.*?{re.escape(ent)}.*?\]\]', ent, text)
    return text

def generate_hebrew_article(ticker, entity_analysis, vocabulary, original_text=None, key_points=None):
    """Generate Hebrew article from entity analysis, with entity protection and relevant vocabulary"""
    try:
        # Extract entities from original text
        entities = list(extract_entities(original_text)) if original_text else []
        entity_list = ', '.join(sorted(set(entities)))
        # Mark entities in the text for the LLM
        marked_text = mark_entities(original_text, entities) if original_text else ''
        # ×‘× ×” snippet ×©×œ ××™×œ×•×Ÿ ×¨×œ×•×•× ×˜×™
        context_for_vocab = original_text or (" ".join([kp['text'] for kp in key_points]) if key_points else '')
        relevant_vocab = load_hebrew_vocabulary(context_text=context_for_vocab)
        # Create prompt with specific instructions to avoid repetitive openings and protect entities
        prompt = f"""××ª×” ××•××—×” ×œ×›×ª×™×‘×ª ×›×ª×‘×•×ª ×¤×™× × ×¡×™×•×ª ×‘×¢×‘×¨×™×ª. ×›×ª×•×‘ ×›×ª×‘×” ××§×¦×•×¢×™×ª ×•××¢× ×™×™× ×ª ×¢×œ {ticker}.

**×—×©×•×‘ ×××•×“ - ×”×•×¨××•×ª ×œ×›×ª×™×‘×”:**
- ××œ ×ª×ª×¨×’× ×©××•×ª ×©×œ ×—×‘×¨×•×ª, ×ª×¨×•×¤×•×ª, ×× ×©×™×, ×¡×™××•×œ×™×, ××•× ×—×™× ××§×¦×•×¢×™×™× â€“ ×”×©××¨ ××•×ª× ×‘×× ×’×œ×™×ª ×›×¤×™ ×©××•×¤×™×¢×™× ×‘×˜×§×¡×˜ ×”××§×•×¨×™.
- ×× ××•×¤×™×¢×” ××™×œ×” ×‘×¡×•×’×¨×™×™× ××¨×•×‘×¢×™× [[...]], ××œ ×ª×™×’×¢ ×‘×”.
- ×œ×”×œ×Ÿ ×¨×©×™××ª ×™×©×•×™×•×ª ×©×™×© ×œ×©××¨ ×‘×× ×’×œ×™×ª: {entity_list}
- ×›××©×¨ ××ª×” ××ª×¨×’× ××•× ×— ××§×¦×•×¢×™, ×‘×“×•×§ ×‘××™×œ×•×Ÿ ×”××¦×•×¨×£ ×•×‘×—×¨ ××ª ×”×ª×¨×’×•× ×”××ª××™× ×‘×™×•×ª×¨. ××œ ×ª××¦×™× ×ª×¨×’×•××™× â€“ ×”×©×ª××© ×¨×§ ×‘××” ×©××•×¤×™×¢ ×‘××™×œ×•×Ÿ ×›×©××¤×©×¨.
- ×‘× ×” ×¤×ª×™×—×” ××¢× ×™×™× ×ª ×‘×”×ª×‘×¡×¡ ×¢×œ ×”××™×“×¢ ×”×¡×¤×¦×™×¤×™ ×©×ª×§×‘×œ
- ×”×ª×—×œ ×¢× ×”××™×“×¢ ×”×›×™ ××¢× ×™×™×Ÿ ××• ×—×©×•×‘ ××”× ×ª×•× ×™×: ×¡×˜×˜×™×¡×˜×™×§×” ××¨×©×™××”, ××™×¨×•×¢ ×—×©×•×‘, ×ª×—×–×™×ª ××¢× ×™×™× ×ª, ××• × ×™×ª×•×— ××¤×ª×™×¢
- ×›×ª×•×‘ ×‘×©×¤×” ××§×¦×•×¢×™×ª ××š × ×’×™×©×”
- ×”×©×ª××© ×‘××•× ×—×™× ×¤×™× × ×¡×™×™× ×¢×‘×¨×™×™× ××ª××™××™×

**××™×“×¢ ×œ× ×™×ª×•×—:**
{entity_analysis}

**××™×œ×•×Ÿ ××•× ×—×™× ×¨×œ×•×•× ×˜×™×™×:**
{relevant_vocab}

**×˜×§×¡×˜ ××§×•×¨×™ ××¡×•××Ÿ:**
{marked_text}

**××‘× ×” ×”×›×ª×‘×”:**
1. ×›×•×ª×¨×ª ×¨××©×™×ª ××¢× ×™×™× ×ª
2. ×¤×ª×™×—×” ××¢× ×™×™× ×ª ×”××‘×•×¡×¡×ª ×¢×œ ×”××™×“×¢ ×”×¡×¤×¦×™×¤×™ (×œ× ××©×¢×××ª!)
3. × ×™×ª×•×— ×”×’×•×¨××™× ×”×—×™×•×‘×™×™×
4. × ×™×ª×•×— ×”×’×•×¨××™× ×”×©×œ×™×œ×™×™×  
5. × ×™×ª×•×— ×”×’×•×¨××™× ×”× ×™×™×˜×¨×œ×™×™×
6. ×¡×™×›×•× ×•×ª×—×–×™×ª

**×”×•×¨××•×ª ×œ×¤×ª×™×—×”:**
- ×§×¨× ××ª ×›×œ ×”××™×“×¢ ×•×–×”×” ××ª ×”× ×ª×•×Ÿ ×”×›×™ ××¢× ×™×™×Ÿ ××• ×—×©×•×‘
- ×”×ª×—×œ ×¢× ××•×ª×• × ×ª×•×Ÿ: ××—×•×– ×©×™× ×•×™, ×ª×—×–×™×ª ×× ×œ×™×¡×˜, ××™×¨×•×¢ ×—×©×•×‘, ××• ×¡×˜×˜×™×¡×˜×™×§×” ××¨×©×™××”
- ××œ ×ª×¤×ª×— ×¢× ××©×¤×˜×™× ×›×œ×œ×™×™× - ×”×ª×—×œ ×¢× ××©×”×• ×¡×¤×¦×™×¤×™ ×•××¢× ×™×™×Ÿ

×›×ª×•×‘ ×›×ª×‘×” ××§×¦×•×¢×™×ª ×•××¢× ×™×™× ×ª ×‘×¢×‘×¨×™×ª:"""
        response = requests.post(
            "http://localhost:11434/api/generate",
            json={
                "model": LLM_MODEL_SETTINGS['model_name'],
                "prompt": prompt,
                "options": {
                    "num_predict": LLM_OUTPUT_SETTINGS.get("default_max_tokens", 5000),  # Use config value
                    "temperature": LLM_MODEL_SETTINGS.get('temperature', 0.7),
                    "top_p": LLM_MODEL_SETTINGS.get('top_p', 0.9),
                },
                "stream": False
            },
            timeout=300,  # Increased from 120 to 300 seconds
        )
        response.raise_for_status()
        article = parse_ollama_response(response)
        # Restore entities in the output
        article = restore_marked_entities(article, entities)
        return article
    except Exception as e:
        logger.error(f"Error generating Hebrew article: {e}")
        raise


def improve_hebrew_article(article_text, ticker_info, vocabulary_examples=None, max_tokens=None):
    """
    ×©×œ×‘ 2: ×¢×¨×™×›×”, ×ª×™×§×•×Ÿ ×“×§×“×•×§, ×ª×—×‘×™×¨, ×”×§×©×¨, ×ª×¨×’×•×, ×ª×•×š ×©×™××•×© ×‘×“×•×’×××•×ª ××”××™×œ×•×Ÿ ×‘××™×“×ª ×”×¦×•×¨×š
    """
    prompt = f"""×¢×¨×•×š ××ª ×”××××¨ ×”×‘× ×›×š ×©×™×”×™×” ×ª×§× ×™, ××§×¦×•×¢×™, ×‘×¨×•×¨, ×•×¢× ×“×§×“×•×§ × ×›×•×Ÿ ×‘×¢×‘×¨×™×ª.

**×”×•×¨××•×ª ×¢×¨×™×›×”:**
1. ×‘×“×•×§ ×“×§×“×•×§ ×¢×‘×¨×™ × ×›×•×Ÿ (×”×ª×××” ×‘×™×Ÿ × ×•×©× ×œ×¤×•×¢×œ, ×–×›×¨/× ×§×‘×”, ×™×—×™×“/×¨×‘×™×)
2. ×ª×¨×’× ××•× ×—×™× ×¤×™× × ×¡×™×™× ×œ×¢×‘×¨×™×ª ××“×•×™×§×ª ×•××§×¦×•×¢×™×ª
3. ×•×“× ×©×›×œ ×”× ×ª×•× ×™× ×”××“×•×™×§×™× (××—×•×–×™×, ××¡×¤×¨×™×, ×ª××¨×™×›×™×) × ×©××¨×™×
4. ×©×¤×¨ × ×™×¡×•×—×™× ×œ×¢×‘×¨×™×ª ×˜×‘×¢×™×ª ×•×–×•×¨××ª
5. ×‘×“×•×§ ×”×ª×××” ×œ×•×’×™×ª ×‘×™×Ÿ ××©×¤×˜×™×
6. ×”×•×¡×£ ××• ×ª×™×§×Ÿ ×”×“×’×©×•×ª (**×˜×§×¡×˜**) ×œ××™×œ×•×ª ××¤×ª×—
7. ×•×“× ×©×™××•×© × ×›×•×Ÿ ×‘×¡×™×× ×™ ×¤×™×¡×•×§ ×¢×‘×¨×™×™×

**×“×•×’×××•×ª ×œ×©×™×¤×•×¨:**
- "××•×¤×˜×™××™×•×ª ×‘×©×•×•×§×™×" â†’ "×—×•×–×§ ×”×©×•×•×§×™×"
- "××›×¡×™ ×¡×—×¨ ××•×¢×™×œ×™×" â†’ "××›×¡×™× ××•×¢×™×œ×™×"  
- "×”×—××¦×ª ×”×–×“×× ×•×™×•×ª" â†’ "×”××ª× ×” ×œ×”×–×“×× ×•×™×•×ª"
- "×”×¨×—×‘×ª × ×•×›×—×•×ª" â†’ "×”×¨×—×‘×ª ×”× ×•×›×—×•×ª"

**××™×œ×•×Ÿ ×¢×–×¨×” (×× ×–××™×Ÿ):**
{vocabulary_examples if vocabulary_examples else "××™×Ÿ ××™×œ×•×Ÿ ×–××™×Ÿ"}

**××××¨ ×œ×¢×¨×™×›×”:**
{article_text}"""
    response = requests.post(
        "http://localhost:11434/api/generate",
        json={
            "model": LLM_MODEL_SETTINGS['model_name'],
            "prompt": prompt,
            "options": {
                "num_predict": max_tokens or LLM_OUTPUT_SETTINGS.get("default_max_tokens", 5000),  # Use config value
                "temperature": LLM_MODEL_SETTINGS.get('temperature', 0.7),
                "top_p": LLM_MODEL_SETTINGS.get('top_p', 0.9),
            },
            "stream": False
        },
        timeout=300,  # Increased from 120 to 300 seconds
    )
    response.raise_for_status()
    return parse_ollama_response(response)


def process_with_contextual_prompt(text_block, ticker_info, metadata_path=None, max_tokens=None, original_text=None):
    """
    ×©×œ×‘ 1: ×™×¦×™×¨×ª ××××¨ ××§×¦×•×¢×™ ×‘×¢×‘×¨×™×ª (LLM)
    ×©×œ×‘ 2: ×¢×¨×™×›×”, ×ª×™×§×•×Ÿ ×“×§×“×•×§, ×ª×—×‘×™×¨, ×”×§×©×¨, ×ª×¨×’×•×, ×ª×•×š ×©×™××•×© ×‘×“×•×’×××•×ª ××”××™×œ×•×Ÿ
    """
    # Load entity analysis if available
    entity_analysis = ""
    if metadata_path and os.path.exists(metadata_path):
        try:
            with open(metadata_path, 'r', encoding='utf-8') as f:
                entity_data = json.load(f)
                if 'compact_analysis' in entity_data:
                    # Parse the compact_analysis JSON string
                    compact_data = json.loads(entity_data['compact_analysis'])
                    # Format it nicely for the LLM
                    entity_analysis = f"""
**× ×™×ª×•×— ×™×©×•×™×•×ª ×¢×‘×•×¨ {compact_data.get('ticker', '')}:**

**×—×‘×¨×•×ª ××•×–×›×¨×•×ª:** {', '.join(compact_data.get('companies', [])[:5])}
**×× ×©×™× ××•×–×›×¨×™×:** {', '.join(compact_data.get('people', [])[:3])}
**×ª×—×•×:** {compact_data.get('industry', '×œ× ×™×“×•×¢')}
**×¨×’×© ×›×œ×œ×™:** {compact_data.get('sentiment', {}).get('overall', '× ×™×™×˜×¨×œ×™')}

**× ×§×•×“×•×ª ××¤×ª×—:**
{chr(10).join([f"â€¢ {point.get('text', '')[:200]}..." for point in compact_data.get('key_points', [])[:3]])}

**×¡×›×•××™ ×›×¡×£:** {', '.join(compact_data.get('money_amounts', [])[:5])}
**×ª××¨×™×›×™× ×—×©×•×‘×™×:** {', '.join(compact_data.get('important_dates', [])[:3])}
**××™×œ×•×ª ××¤×ª×— ×¤×™× × ×¡×™×•×ª:** {', '.join(compact_data.get('financial_keywords', [])[:5])}
"""
                else:
                    entity_analysis = str(entity_data)
        except Exception as e:
            logger.warning(f"Could not load entity analysis: {e}")
            entity_analysis = ""
    
    # Load vocabulary
    vocabulary = load_hebrew_vocabulary()
    
    # Generate Hebrew article (pass original_text for entity protection)
    hebrew_article = generate_hebrew_article(ticker_info.get('ticker'), entity_analysis, vocabulary, original_text=original_text)
    
    # Improve the article
    improved_article = improve_hebrew_article(hebrew_article, ticker_info, vocabulary_examples=vocabulary, max_tokens=max_tokens)
    return improved_article

# ×“×•×’××” ×œ×©×™××•×©
if __name__ == '__main__':
    import sys
    import argparse
    parser = argparse.ArgumentParser(description='Process MarketBit original txt to Hebrew HTML article.')
    parser.add_argument('input', help='Path to original txt file')
    parser.add_argument('--output', help='Output HTML/text file path', default=None)
    parser.add_argument('--ticker', help='Ticker symbol (optional)', default=None)
    parser.add_argument('--text', action='store_true', help='Output plain text instead of HTML')
    args = parser.parse_args()
    try:
        result = process_article_file(args.input, args.output, args.ticker, as_html=not args.text)
        print(result[:1000] + ('...\n[truncated]' if len(result) > 1000 else ''))
    except Exception as e:
        logger.error(f"âŒ {e}")
        print(f"Error: {e}")