import requests
import json

def process_with_gemma(original_text, ticker):
    """
    Process text with Ollama using aya-expanse:8b model
    """
    
    prompt = f"""××ª×” ×›×•×ª×‘ ××˜×¢× ×’×•×£ ××—×§×¨ ×¤×™× × ×¡×™ ×¢×¦×××™ ×‘×©× "Hippopotamus Research".

×”××˜×¨×”: ×œ× ×ª×— ×ª× ×•×¢×ª ×× ×™×” ×™×•××™×ª ×‘××•×¤×Ÿ ×¡×™×‘×ª×™, ××§×¦×•×¢×™ ×•×××™×Ÿ.  
×”×›×ª×™×‘×” ××™× ×” ×©×™×•×•×§×™×ª, ××™× ×” ×¨×’×©×™×ª ×•××™× ×” ×›×œ×œ×™×ª â€“ ××œ× ×× ×œ×™×˜×™×ª, ××“×•×™×§×ª, ×•××‘×•×¡×¡×ª ×ª×¦×¤×™×•×ª.  
×›×œ ×ª× ×•×¢×” ××•×¡×‘×¨×ª ×‘×§×¤×“× ×•×ª ×“×¨×š ××¤×ª × ×™×ª×•×— ×”×›×•×œ×œ×ª: ×”×¦×”×¨×•×ª ×”× ×”×œ×”, ××•×¡×“×•×ª, ×“×™×‘×™×“× ×“, ×¨×’×•×œ×¦×™×”, ××’××•×ª ×¡×§×˜×•×¨×™××œ×™×•×ª, ××©×¤×˜×™×, ×©×™×— ×¦×™×‘×•×¨×™, ×•××™×“×¢ ×¤× ×™×.

××§×•×¨×•×ª ×”××™×“×¢: MarketBeat, Seeking Alpha, Yahoo Finance, Benzinga, TipRanks

ğŸ”¸ **×›×ª×™×‘×” ×‘×¡×’× ×•×Ÿ ××•×¡×“×™ ×‘×›×™×¨** â€“ ×›××™×œ×• ××ª×” ×× ×”×œ ××—×œ×§×ª ××—×§×¨ ×‘×’×•×£ ×”×©×§×¢×•×ª ×¢× ×§.  
ğŸ”¸ **×¡×’× ×•×Ÿ**: ×¨×”×•×˜, ××—×•×œ×§ ×œ×¤×¡×§××•×ª, ×—×›×, ×—×“, × ×§×™ ××‘××–×–×™×.  
ğŸ”¸ **×§×©×¨ ×¤× ×™××™**: ×›×œ ×¤×¡×§×” ××—×•×‘×¨×ª ×¨×¢×™×•× ×™×ª ×œ×¤×¡×§×” ×©××—×¨×™×”, ×›×š ×©×”×§×¨×™××” ×–×•×¨××ª ×•×œ× ××•×¡×£ ×©×œ × ×§×•×“×•×ª.

**×›×œ ×¤×¡×§×” ×¦×¨×™×›×”:**
- ×œ×¢×¡×•×§ ×‘×§×˜×’×•×¨×™×” ××—×ª ××¨×›×–×™×ª (×œ×“×•×’××”: ×”× ×”×œ×”, ××•×¡×“×•×ª, ×¨×’×•×œ×¦×™×”, ×¦×™×‘×•×¨).
- ×œ×”×›×™×œ ××©×¤×˜×™ ×¢×•××§ ×•×œ× ×¨×§ ×ª×™××•×¨.
- ×œ×”××™×¨ ××™×“×¢ ×™×‘×© ×œ× ×¨×˜×™×‘ ××™× ×¤×•×¨××˜×™×‘×™ ××¢× ×™×™×Ÿ.
- ×œ× ×œ×”××¦×™× ×¢×•×‘×“×•×ª â€“ ×¨×§ ×œ×”×¡×‘×™×¨ ×œ×¢×•××§ ××ª ×”× ×ª×•× ×™× ×©× ××¡×¨×•.

**×¡×™×•× ×”×“×•×—:**  
×œ×¡×›× ××ª ×”××¦×‘ ×ª×•×š ×¦×™×•×Ÿ ××™×–×•×Ÿ ×‘×™×Ÿ ×›×•×—×•×ª ×—×™×•×‘×™×™× (×›×’×•×Ÿ ×ª×–×¨×™×, ×¦××™×—×”, ××•×¡×“×•×ª) ×œ×›×•×—×•×ª ×××–× ×™× ××• ×©×œ×™×œ×™×™× (××©×¤×˜×™×, ×¡×‘×™×‘×”, ×¨×’×•×œ×¦×™×”), ×•×œ×¦×™×™×Ÿ ×©× ×“×¨×© ×”××©×š ××¢×§×‘.

× ×ª×— ××ª ×”××™×“×¢ ×”×‘× ×¢×‘×•×¨ {ticker}:

{original_text}"""
    
    try:
        print("ğŸ”„ Processing with Ollama...")
        print(f"ğŸ“ Prompt length: {len(prompt)} characters")
        
        url = "http://localhost:11434/api/generate"
        payload = {
            "model": "aya-expanse:8b",
            "prompt": prompt,
            "stream": True,
            "options": {
                "temperature": 0.9,
                "top_p": 0.95,
                "num_predict": 2000
            }
        }
        
        response = requests.post(url, json=payload, timeout=120, stream=True)
        
        if response.status_code == 200:
            full_response = ""
            chunk_count = 0
            
            for line in response.iter_lines():
                if line:
                    try:
                        data = json.loads(line.decode('utf-8'))
                        if 'response' in data:
                            full_response += data['response']
                            chunk_count += 1
                        if data.get('done', False):
                            break
                    except json.JSONDecodeError:
                        continue
            
            print(f"ğŸ“ Received {chunk_count} chunks from Ollama")
            print(f"ğŸ“ Response length: {len(full_response)} characters")
            print(f"ğŸ“ Original length: {len(original_text)} characters")
            
            # Check if the response is different from original
            if full_response.strip() == original_text.strip():
                print("âš ï¸ Model returned original text - using fallback")
                return fallback_processing(original_text, ticker)
            
            # Check if response is too short (might be incomplete)
            if len(full_response) < len(original_text) * 0.5:
                print("âš ï¸ Response too short - using fallback")
                return fallback_processing(original_text, ticker)
            
            # Check if response contains the original text (might be just echoing)
            if original_text in full_response:
                print("âš ï¸ Response contains original text - using fallback")
                return fallback_processing(original_text, ticker)
            
            print("âœ… Successfully processed with Ollama")
            return full_response
            
        else:
            print(f"âŒ Ollama Error: {response.status_code}")
            print(f"Response: {response.text}")
            return fallback_processing(original_text, ticker)
            
    except requests.exceptions.ConnectionError:
        print("âŒ Cannot connect to Ollama - make sure it's running")
        return fallback_processing(original_text, ticker)
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        return fallback_processing(original_text, ticker)

def fallback_processing(original_text, ticker):
    """
    Fallback processing if Ollama is not available
    """
    print("âš ï¸ Using fallback processing...")
    
    # Simple text processing to make it look like institutional research
    lines = original_text.split('\n')
    processed_lines = []
    
    # Add institutional header
    processed_lines.append(f"# ×“×•×— × ×™×ª×•×— ×¡×™×‘×ª×™×•×ª - {ticker}")
    processed_lines.append("")
    processed_lines.append("**Hippopotamus Research**")
    processed_lines.append("")
    
    # Process each line
    for line in lines:
        line = line.strip()
        if line and not line.startswith("×¤×•×¨×¡×") and not line.startswith("× ×•×¦×¨"):
            if line.startswith("×’×•×¨××™× ×¢×™×§×¨×™×™×"):
                processed_lines.append("## ×’×•×¨××™× ×¢×™×§×¨×™×™×")
            elif line.startswith("×¤×™×™×¤×¨") or line.startswith("×¨×•×ª'") or line.startswith("×§×œ×™×˜×”"):
                processed_lines.append(f"â€¢ {line}")
            else:
                processed_lines.append(line)
    
    # Add institutional conclusion
    processed_lines.append("")
    processed_lines.append("## ×¡×™×›×•× ×•× ×™×ª×•×—")
    processed_lines.append("")
    processed_lines.append("×”×“×•×— ×”× ×•×›×—×™ ××¦×™×’ × ×™×ª×•×— ×¡×™×‘×ª×™ ×©×œ ×ª× ×•×¢×•×ª ×”×× ×™×”. × ×“×¨×© ×”××©×š ××¢×§×‘ ××—×¨ ×”×ª×¤×ª×—×•×™×•×ª ×¢×ª×™×“×™×•×ª.")
    
    result = '\n'.join(processed_lines)
    print(f"ğŸ“ Fallback processing complete: {len(result)} characters")
    return result
