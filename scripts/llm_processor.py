import pandas as pd
import subprocess
import json as pyjson
import re
import os
import sys

# ×”×•×¡×¤×ª ×”× ×ª×™×‘ ×”×¨××©×™ ×œ××¢×¨×›×ª ×›×“×™ ×©× ×•×›×œ ×œ×§×¨×•× ××ª ×§×•×‘×¥ ×”-CSV
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# ×§×¨×™××ª ××™×¤×•×™ ×˜×™×§×¨×™× ×œ×¡×§×˜×•×¨×™× - ×¢× ×˜×™×¤×•×œ ×‘××§×¨×” ×©×”×§×•×‘×¥ ×œ× ×§×™×™×
sector_map = {}
csv_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "data", "flat-ui__data-Thu Jun 19 2025.csv")
if os.path.exists(csv_path):
    try:
        sector_map_df = pd.read_csv(csv_path)
        sector_map = dict(zip(sector_map_df['Tickers'], sector_map_df['GICS Sector']))
    except Exception as e:
        print(f"âš ï¸ Warning: Could not load sector mapping: {e}")
else:
    print("âš ï¸ Warning: CSV file not found, sector mapping will be empty")

# ×¤×¨×•××¤×˜ ××¢×•×“×›×Ÿ ×œ×¤×™ ×“×¨×™×©×•×ª ×”××©×ª××© â€“ ×©×›×ª×•×‘ ×‘×œ×‘×“, ×œ×œ× ×¤×¨×©× ×•×ª
def generate_prompt(original_text: str, ticker_info=None):
    company = ticker_info.get("Security") if ticker_info else ""
    sector_name = ticker_info.get("GICS Sector") if ticker_info else ""

    prompt = f"""
××ª×” ×›×•×ª×‘ ×›×ª×‘×” ××§×¦×•×¢×™×ª ×•××¨×ª×§×ª ×¢×‘×•×¨ ×’×•×£ ××—×§×¨ ×¢×¦×××™ ×‘×©× "Hippopotamus Research".

ğŸ¯ ××˜×¨×ª×š:
×œ×™×¦×•×¨ ×›×ª×‘×” ××¨×•×›×”, ××¡×•×’× × ×ª ×•××¢× ×™×™× ×ª ×”××›×¡×” ××ª ×›×œ × ×§×•×“×•×ª ×”××¤×ª×— ×©× ××¡×¨×•, ×¢× ×¡×’× ×•×Ÿ ×›×ª×™×‘×” ×¡×™×¤×•×¨×™ ×•××¨×ª×§.

ğŸ“Œ ××” ×©×§×™×‘×œ×ª:
×¨×©×™××ª × ×§×•×“×•×ª ××¤×ª×— (key points) ×¢×œ ×—×‘×¨×” ×›×œ×©×”×™, ×›×œ ××©×¤×˜ ×”×•× × ×§×•×“×ª ×¢× ×™×™×Ÿ × ×¤×¨×“×ª.

ğŸ“Œ ××” ×¢×œ×™×š ×œ×¢×©×•×ª:
1. **×›×ª×•×‘ ×›×ª×‘×” ××¨×•×›×” ×•××§×™×¤×”** - ×”××›×¡×” ××ª ×›×œ × ×§×•×“×•×ª ×”××¤×ª×— ×œ×œ× ×™×•×¦× ××Ÿ ×”×›×œ×œ
2. **×©××•×¨ ×¢×œ ×›×œ ×”× ×ª×•× ×™×** - ××¡×¤×¨×™×, ×ª××¨×™×›×™×, ×©××•×ª, ×¦×™×˜×•×˜×™× ×•××—×™×¨×™ ×™×¢×“ ×—×™×™×‘×™× ×œ×”×™×©××¨ ×‘×“×™×•×§ ×›×¤×™ ×©×”×
3. **×¦×•×¨ ××‘× ×” ××§×¦×•×¢×™ ×¢× ×¡×™××•×Ÿ ×‘×¨×•×¨** - ×”×©×ª××© ×‘××‘× ×” ×”×‘×:
   - ×›×•×ª×¨×ª ×¨××©×™×ª: ×”×ª×—×œ ××ª ×”×©×•×¨×” ×‘-#TITLE#
   - ×›×•×ª×¨×ª ××©× ×”: ×”×ª×—×œ ××ª ×”×©×•×¨×” ×‘-#SUBTITLE#
   - ×¤×¡×§×” ×¨×’×™×œ×”: ×”×ª×—×œ ××ª ×”×©×•×¨×” ×‘-#PARA#
4. **×¡×’× ×•×Ÿ ×›×ª×™×‘×” ×¡×™×¤×•×¨×™ ×•××¨×ª×§** - ×›×ª×•×‘ ×‘×¦×•×¨×” ×©××•×©×›×ª ×¢× ×™×™×Ÿ, ×¢×:
   - ×›×•×ª×¨×•×ª ××¢× ×™×™× ×•×ª ×•××¢×•×¨×¨×•×ª ×¡×§×¨× ×•×ª
   - ×©××œ×•×ª ×¨×˜×•×¨×™×•×ª ×©××¢×•×¨×¨×•×ª ××—×©×‘×”
   - ×—×™×‘×•×¨×™× ×œ×•×’×™×™× ×•×–×•×¨××™× ×‘×™×Ÿ ×”×¤×¡×§××•×ª
   - ×ª×™××•×¨×™× ×—×™×™× ×•××¨×ª×§×™×
   - ×©×™××•×© ×‘××˜××¤×•×¨×•×ª ×•××©×—×§×™ ××™×œ×™× ××ª××™××™×
5. **××§×•×¨×™×•×ª** - ××œ ×ª×—×–×•×¨ ×¢×œ ×›×•×ª×¨×•×ª ××• ××©×¤×˜×™× ×–×”×™×, ×©××•×¨ ×¢×œ ×’×™×•×•×Ÿ ×•×—×“×©× ×•×ª

ğŸ­ ×¡×’× ×•×Ÿ ×”×›×ª×™×‘×” ×”×¨×¦×•×™:
- **×›×•×ª×¨×•×ª ××¢× ×™×™× ×•×ª**: "×”××™×¨×•×¥ ×œ××¨×—×‘ ×”×¡×™×™×‘×¨", "×”××©×—×§ ×”×’×“×•×œ ×©×œ ×”×©×•×•×§×™×"
- **×©××œ×•×ª ×¨×˜×•×¨×™×•×ª**: "×”×× ×–×”×• ×¨×¥ ×¡×•×¡ ×× ×¦×— ××• ××©×œ×™×” ×§×¦×¨×ª ×˜×•×•×—?"
- **×ª×™××•×¨×™× ×—×™×™×**: "×”×× ×™×” ×”×’×™×¢×” ×œ×©×™××™× ×—×“×©×™×, ×¢× ×©×™××™ 52 ×©×‘×•×¢×•×ª ×•×©×™××™ ×›×œ ×”×–×× ×™×"
- **×—×™×‘×•×¨×™× ×–×•×¨××™×**: "×‘×¢×•×“ ×©×”×¢×•×œ× ××ª××•×“×“ ×¢×...", "×××—×•×¨×™ ×”×—×’×™×’×” ×”×–××ª..."
- **××˜××¤×•×¨×•×ª ××ª××™××•×ª**: "×”××ª×—×™× ×”×’×™××•×¤×•×œ×™×˜×™×™× ××©××©×™× ×›×“×œ×§ × ×•×¡×£"

âš ï¸ ×›×œ×œ×™× ×—×©×•×‘×™×:
- ××¡×•×¨ ×œ×©× ×•×ª ××£ × ×ª×•×Ÿ ××¡×¤×¨×™ ××• ××™×“×¢ ×—×©×•×‘
- ××¡×•×¨ ×œ×¤×¡×¤×¡ ××£ × ×§×•×“×ª ××¤×ª×— - ×›×œ ×¤×™×¡×ª ××™×“×¢ ×—×™×™×‘×ª ×œ×”×•×¤×™×¢ ×‘×›×ª×‘×”
- ××¡×•×¨ ×œ×”×•×¡×™×£ ××™×“×¢ ×—×“×© ×©×œ× ×”×™×” ×‘××§×•×¨
- ××¡×•×¨ ×œ×‘×¦×¢ × ×™×ª×•×— ××• ×ª×—×–×™×•×ª ××©×œ×š
- ×”×›×ª×‘×” ×—×™×™×‘×ª ×œ×”×™×•×ª × ××× ×” ×œ××™×“×¢ ×”××§×•×¨×™ ××š ××¡×•×’× × ×ª ×‘×›×ª×™×‘×”

âœï¸ ××‘× ×” ×”×›×ª×‘×”:
- ×©×•×¨×” ×¨××©×•× ×”: #TITLE# ×›×•×ª×¨×ª ×¨××©×™×ª ××¢× ×™×™× ×ª
- ×©×•×¨×” ×©× ×™×™×”: #SUBTITLE# ×›×•×ª×¨×ª ××©× ×” ×¨××©×•× ×”
- ×©×•×¨×” ×©×œ×™×©×™×ª: #PARA# ×¤×¡×§×” ×¨××©×•× ×”
- ×•×›×Ÿ ×”×œ××”...

ğŸ” ×—×‘×¨×”: {company}
ğŸ“‚ ×¡×§×˜×•×¨: {sector_name}

**× ×§×•×“×•×ª ×”××¤×ª×— ×”××§×•×¨×™×•×ª:**
===
{original_text}
===

âš ï¸ ×—×©×•×‘ ×××•×“: ×”×—×–×¨ ×˜×§×¡×˜ ×‘×œ×‘×“, ×›×œ ×©×•×¨×” ××ª×—×™×œ×” ×‘××—×“ ××”×‘××™×: #TITLE#, #SUBTITLE#, #PARA#. ××™×Ÿ ×œ×”×—×–×™×¨ ×ª×’×™ HTML, markdown, JSON ××• ×ª×’×™× ××—×¨×™×.
"""
    return prompt

def convert_tagged_text_to_html(text):
    """
    ×”××¨×ª ×˜×§×¡×˜ ××¡×•××Ÿ (#TITLE#, #SUBTITLE#, #PARA#) ×œ-HTML ×ª×§× ×™, ×’× ×× ×”×¡×™××•× ×™× ×‘×××¦×¢ ×©×•×¨×” ××• ×¢× # ××™×•×ª×¨
    """
    # Normalize all markers to canonical form (e.g. ## SUBTITLE# -> #SUBTITLE#)
    text = re.sub(r'#+\s*SUBTITLE#', '#SUBTITLE#', text)
    text = re.sub(r'#+\s*TITLE#', '#TITLE#', text)
    text = re.sub(r'#+\s*PARA#', '#PARA#', text)

    # Split text by markers, keeping the marker in the result
    parts = re.split(r'(#TITLE#|#SUBTITLE#|#PARA#)', text)
    html_lines = []
    current_tag = None
    for part in parts:
        part = part.strip()
        if not part:
            continue
        if part in ['#TITLE#', '#SUBTITLE#', '#PARA#']:
            current_tag = part
        else:
            if current_tag == '#TITLE#':
                html_lines.append(f"<h1>{part}</h1>")
            elif current_tag == '#SUBTITLE#':
                html_lines.append(f"<h2>{part}</h2>")
            elif current_tag == '#PARA#':
                html_lines.append(f"<p>{part}</p>")
            else:
                html_lines.append(f"<p>{part}</p>")
    return '\n'.join(html_lines)

# ×”×¤×¢×œ×ª ××•×“×œ Ollama ×¢× prompt ××¢×•×“×›×Ÿ
def process_with_gemma(original_text, ticker_info=None):
    """
    Process the original text with the LLM (aya-expanse:8b) using ONLY rephrasing and restructuring rules.
    Returns the processed text as a string.
    """
    prompt = generate_prompt(original_text, ticker_info)

    if ticker_info:
        prompt += "\n---\n××™×“×¢ × ×•×¡×£ ×¢×œ ×”×—×‘×¨×”:\n"
        for k, v in ticker_info.items():
            prompt += f"{k}: {v}\n"

    prompt += "\n---\n×”×—×–×¨ ×˜×§×¡×˜ ×‘×œ×‘×“, ×›×œ ×©×•×¨×” ××ª×—×™×œ×” ×‘××—×“ ××”×‘××™×: #TITLE#, #SUBTITLE#, #PARA#. ××™×Ÿ ×œ×”×—×–×™×¨ ×ª×’×™ HTML, markdown, JSON ××• ×ª×’×™× ××—×¨×™×."

    try:
        result = subprocess.run(
            ["ollama", "run", "aya-expanse:8b"],
            input=prompt.encode("utf-8"),
            capture_output=True
        )
        output = result.stdout.decode("utf-8").strip()
        print(f"ğŸ” DEBUG: Raw LLM output (first 200 chars): {output[:200]}...")
        # × ×™×§×•×™ ×”×¤×œ×˜ ××›×œ ×¡×•×’×™ JSON ×•×ª×’×™×
        cleaned_output = clean_llm_text(output)
        print(f"ğŸ” DEBUG: After clean_llm_text (first 200 chars): {cleaned_output[:200]}...")
        # ×”×¡×¨×ª ×ª×’×™× ×•×¢×•×‘×“×•×ª ×× ×¢×“×™×™×Ÿ ×§×™×™××™×
        cleaned_output = remove_json_artifacts(cleaned_output)
        print(f"ğŸ” DEBUG: After remove_json_artifacts (first 200 chars): {cleaned_output[:200]}...")
        # ×”××¨×” ××”×¤×•×¨××˜ ×”××¡×•××Ÿ ×œ-HTML
        cleaned_output = convert_tagged_text_to_html(cleaned_output)
        print(f"ğŸ” DEBUG: After convert_tagged_text_to_html (first 200 chars): {cleaned_output[:200]}...")
        print(f"ğŸ” DEBUG: Final output contains '<h': {'<h' in cleaned_output}")
        print(f"ğŸ” DEBUG: Final output contains '<p': {'<p' in cleaned_output}")
        return cleaned_output
    except Exception as e:
        print(f"âŒ Error running ollama: {e}")
        return clean_llm_text("×©×’×™××” ×‘×¢×™×‘×•×“ LLM: " + str(e))

def remove_json_artifacts(text):
    """Remove JSON artifacts, tags, and facts from the text"""
    if not text:
        return text
    
    # ×”×¡×¨×ª JSON ××œ×
    text = re.sub(r'^\s*\{.*?"text":\s*"', '', text, flags=re.DOTALL)
    text = re.sub(r'",\s*"tags":\s*\[.*?\]\s*,\s*"facts":\s*\[.*?\]\s*\}\s*$', '', text, flags=re.DOTALL)
    text = re.sub(r'",\s*"tags":\s*\[.*?\]\s*\}\s*$', '', text, flags=re.DOTALL)
    text = re.sub(r'",\s*"facts":\s*\[.*?\]\s*\}\s*$', '', text, flags=re.DOTALL)
    text = re.sub(r'"\s*\}\s*$', '', text)
    
    # ×”×¡×¨×ª ×ª×’×™× ×•×¢×•×‘×“×•×ª ×‘×•×“×“×™×
    text = re.sub(r',\s*"tags":\s*\[.*?\]', '', text, flags=re.DOTALL)
    text = re.sub(r',\s*"facts":\s*\[.*?\]', '', text, flags=re.DOTALL)
    
    # ×”×¡×¨×ª markdown
    text = re.sub(r'```json\s*', '', text)
    text = re.sub(r'```\s*', '', text)
    
    # × ×™×§×•×™ × ×•×¡×£
    text = re.sub(r'^\s*"', '', text)
    text = re.sub(r'"\s*$', '', text)
    text = re.sub(r'\\n', '\n', text)
    text = re.sub(r'\\"', '"', text)
    
    return text.strip()

def clean_llm_text(text):
    """Clean LLM output from JSON artifacts, HTML tags, markdown symbols, and formatting issues"""
    if not text:
        return text
    
    # Remove JSON structure artifacts
    text = re.sub(r'^\s*\{\s*', '', text)
    text = re.sub(r'\s*\}\s*$', '', text)
    text = re.sub(r'^\s*"text":\s*"', '', text)
    text = re.sub(r'^\s*"":\s*"', '', text)
    text = re.sub(r'"\s*,\s*"tags":\s*\[\s*\]\s*$', '', text)
    text = re.sub(r'"\s*$', '', text)
    
    # Remove HTML tags
    text = re.sub(r'<[^>]+>', '', text)
    
    # Remove markdown symbols
    text = re.sub(r'^#+\s*', '', text)  # Remove markdown headers
    text = re.sub(r'\*\*([^*]+)\*\*', r'\1', text)  # Remove bold markdown
    text = re.sub(r'\*([^*]+)\*', r'\1', text)  # Remove italic markdown
    
    # Clean up newlines and whitespace
    text = re.sub(r'\\n', '\n', text)
    text = re.sub(r'\n\s*\n', '\n\n', text)  # Normalize paragraph breaks
    text = re.sub(r' +', ' ', text)  # Normalize spaces
    
    return text.strip() 