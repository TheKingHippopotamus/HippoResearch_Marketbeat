import pandas as pd
import subprocess
import json as pyjson
import re
import os
import sys
import logging

# Setup logging
logger = logging.getLogger(__name__)

# ×”×•×¡×¤×ª ×”× ×ª×™×‘ ×”×¨××©×™ ×œ××¢×¨×›×ª ×›×“×™ ×©× ×•×›×œ ×œ×§×¨×•× ××ª ×§×•×‘×¥ ×”-CSV
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# ×§×¨×™××ª ××™×¤×•×™ ×˜×™×§×¨×™× ×œ×¡×§×˜×•×¨×™× - ×¢× ×˜×™×¤×•×œ ×‘××§×¨×” ×©×”×§×•×‘×¥ ×œ× ×§×™×™×
sector_map = {}
csv_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "data", "flat-ui__data-Thu Jun 19 2025.csv")
if os.path.exists(csv_path):
    try:
        sector_map_df = pd.read_csv(csv_path)
        sector_map = dict(zip(sector_map_df['Tickers'], sector_map_df['GICS Sector']))
    except Exception as e:
        logger.warning(f"âš ï¸ Warning: Could not load sector mapping: {e}")
else:
    logger.warning("âš ï¸ Warning: CSV file not found, sector mapping will be empty")

# ×¤×¨×•××¤×˜ ××¢×•×“×›×Ÿ ×œ×¤×™ ×“×¨×™×©×•×ª ×”××©×ª××© â€“ ×©×›×ª×•×‘ ×‘×œ×‘×“, ×œ×œ× ×¤×¨×©× ×•×ª
def generate_prompt(original_text: str, ticker_info=None):
    company = ticker_info.get("Security") if ticker_info else ""
    sector_name = ticker_info.get("GICS Sector") if ticker_info else ""

    prompt = f"""
××ª×” ×›×•×ª×‘ ×›×ª×‘×” ××§×¦×•×¢×™×ª ×•××¨×ª×§×ª ×¢×‘×•×¨ ×’×•×£ ××—×§×¨ ×¢×¦×××™ ×‘×©× "Hippopotamus Research".

ğŸ¯ ××˜×¨×ª×š:
×œ×™×¦×•×¨ ×›×ª×‘×” ××¨×•×›×”, ××¡×•×’× × ×ª ×•××¢× ×™×™× ×ª ×”××›×¡×” ××ª ×›×œ × ×§×•×“×•×ª ×”××¤×ª×— ×©× ××¡×¨×• - ×œ×¢×•×œ× ××œ ×ª×“×œ×’ ××• ×ª×©××™×˜ ××™×“×¢ ××”××™×“×¢ ×©×§×™×‘×œ×ª ! ××ª×” ×—×™×™×‘ ×œ×”×©×ª××© ×‘×›×•×œ×• ! , ×¢× ×¡×’× ×•×Ÿ ×›×ª×™×‘×” ×¡×™×¤×•×¨×™ ×•××¨×ª×§.

ğŸ“Œ ××” ×©×§×™×‘×œ×ª:
×¨×©×™××ª × ×§×•×“×•×ª ××¤×ª×— (key points) ×¢×œ ×—×‘×¨×” ×›×œ×©×”×™, ×›×œ ××©×¤×˜ ×”×•× × ×§×•×“×ª ×¢× ×™×™×Ÿ × ×¤×¨×“×ª.

ğŸ“Œ ××” ×¢×œ×™×š ×œ×¢×©×•×ª:
1. **×›×ª×•×‘ ×›×ª×‘×” ××¨×•×›×” ×•××§×™×¤×”** - ×”××›×¡×” ××ª ×›×œ × ×§×•×“×•×ª ×”××¤×ª×— ×œ×œ× ×™×•×¦× ××Ÿ ×”×›×œ×œ
2. **×©××•×¨ ×¢×œ ×›×œ ×”× ×ª×•× ×™×** - ××¡×¤×¨×™×, ×ª××¨×™×›×™×, ×©××•×ª, ×¦×™×˜×•×˜×™× ×•××—×™×¨×™ ×™×¢×“ ×—×™×™×‘×™× ×œ×”×™×©××¨ ×‘×“×™×•×§ ×›×¤×™ ×©×”×
3. **×¦×•×¨ ××‘× ×” ××§×¦×•×¢×™ ×¢× ×¡×™××•×Ÿ ×‘×¨×•×¨** - ×”×©×ª××© ×‘××‘× ×” ×”×‘×:
   - ×›×•×ª×¨×ª ×¨××©×™×ª: ×”×ª×—×œ ××ª ×”×©×•×¨×” ×‘-#TITLE#
   - ×›×•×ª×¨×ª ××©× ×”: ×”×ª×—×œ ××ª ×”×©×•×¨×” ×‘-#SUBTITLE#
   - ×¤×¡×§×” ×¨×’×™×œ×”: ×”×ª×—×œ ××ª ×”×©×•×¨×” ×‘-#PARA#
4. **×¡×’× ×•×Ÿ ×›×ª×™×‘×” ×™×¦×™×¨×ª×™, ××’×•×•×Ÿ ×•××¤×ª×™×¢** - ×›×œ ×›×ª×‘×” ×—×™×™×‘×ª ×œ×”×™×•×ª ×‘×¡×’× ×•×Ÿ ×©×•× ×”, ××§×•×¨×™, ×•×—×“×©× ×™. ××œ ×ª×—×–×•×¨ ×¢×œ ×›×•×ª×¨×•×ª, ×‘×™×˜×•×™×™×, ××• ××‘× ×” ××”×›×ª×‘×•×ª ×”×§×•×“××•×ª. ×”×©×ª××© ×‘××’×•×•×Ÿ ×¨×—×‘ ×©×œ ×¡×’× ×•× ×•×ª, ××˜××¤×•×¨×•×ª, ×©××œ×•×ª ×¨×˜×•×¨×™×•×ª, ×•×“×™××•×™×™×. ×”×¤×ª×¢ ××ª ×”×§×•×¨× ×‘×›×œ ×›×ª×‘×” ××—×“×©. ××œ ×ª×©×ª××© ×‘×“×•×’×××•×ª ××”×•×¨××•×ª ××œ×• â€“ ×”××¦× ×‘×¢×¦××š!
5. **××§×•×¨×™×•×ª** - ××œ ×ª×—×–×•×¨ ×¢×œ ×›×•×ª×¨×•×ª ××• ××©×¤×˜×™× ×–×”×™×, ×©××•×¨ ×¢×œ ×’×™×•×•×Ÿ ×•×—×“×©× ×•×ª

âš ï¸ ×›×œ×œ×™× ×—×©×•×‘×™×:
- ××¡×•×¨ ×œ×©× ×•×ª ××£ × ×ª×•×Ÿ ××¡×¤×¨×™ ××• ××™×“×¢ ×—×©×•×‘
- ××¡×•×¨ ×œ×¤×¡×¤×¡ ××£ × ×§×•×“×ª ××¤×ª×— - ×›×œ ×¤×™×¡×ª ××™×“×¢ ×—×™×™×‘×ª ×œ×”×•×¤×™×¢ ×‘×›×ª×‘×”
- ××¡×•×¨ ×œ×”×•×¡×™×£ ××™×“×¢ ×—×“×© ×©×œ× ×”×™×” ×‘××§×•×¨
- ××¡×•×¨ ×œ×‘×¦×¢ × ×™×ª×•×— ××• ×ª×—×–×™×•×ª ××©×œ×š
- ×”×›×ª×‘×” ×—×™×™×‘×ª ×œ×”×™×•×ª × ××× ×” ×œ××™×“×¢ ×”××§×•×¨×™ ××š ××¡×•×’× × ×ª ×‘×›×ª×™×‘×”

ğŸ”¤ ×”× ×—×™×” ×—×©×•×‘×”:
×›×œ ×©× ×©×œ ××•×¡×“, ××ª×¨, ×—×‘×¨×”, ×’×•×£ ××—×§×¨, ×›×œ×™ ×ª×§×©×•×¨×ª (×œ××©×œ: CNBC, Bloomberg, Palantir, Visa, Microsoft, Google, Reuters, MarketBeat) â€“ ×™×© ×œ×›×ª×•×‘ ×‘×× ×’×œ×™×ª ×‘×œ×‘×“, ×’× ×× ×©××¨ ×”×›×ª×‘×” ×‘×¢×‘×¨×™×ª. ××™×Ÿ ×œ×ª×¨×’× ××• ×œ×ª×¢×ª×§ ×©××•×ª ××œ×• ×œ×¢×‘×¨×™×ª.

âœï¸ ××‘× ×” ×”×›×ª×‘×”:
- ×©×•×¨×” ×¨××©×•× ×”: #TITLE# ×›×•×ª×¨×ª ×¨××©×™×ª ××¢× ×™×™× ×ª
- ×©×•×¨×” ×©× ×™×™×”: #SUBTITLE# ×›×•×ª×¨×ª ××©× ×” ×¨××©×•× ×”
- ×©×•×¨×” ×©×œ×™×©×™×ª: #PARA# ×¤×¡×§×” ×¨××©×•× ×”
- ×•×›×Ÿ ×”×œ××”...

ğŸ” ×—×‘×¨×”: {company}
ğŸ“‚ ×¡×§×˜×•×¨: {sector_name}

**× ×§×•×“×•×ª ×”××¤×ª×— ×”××§×•×¨×™×•×ª:**
===
{original_text}
===

âš ï¸ ×—×©×•×‘ ×××•×“: ×”×—×–×¨ ×˜×§×¡×˜ ×‘×œ×‘×“, ×›×œ ×©×•×¨×” ××ª×—×™×œ×” ×‘××—×“ ××”×‘××™×: #TITLE#, #SUBTITLE#, #PARA#. ××™×Ÿ ×œ×”×—×–×™×¨ ×ª×’×™ HTML, markdown, JSON ××• ×ª×’×™× ××—×¨×™×.
"""
    return prompt

def clean_processed_text(text):
    """
    ×× ×§×” ××ª ×”×˜×§×¡×˜ ×”××¢×•×‘×“ ××¡×™××•× ×™× ××™×•×ª×¨×™× ×•×ª×’×™× ×œ× × ×›×•× ×™×
    """
    if not text:
        return text
    
    # ×”×¡×¨×ª ×¡×™××•× ×™× ×¤× ×™××™×™× ×©×œ ×”××¢×¨×›×ª
    text = re.sub(r'TITLE#\s*', '', text)
    text = re.sub(r'SUBTITLE#\s*', '', text)
    text = re.sub(r'PARA#\s*', '', text)
    
    # ×”×¡×¨×ª ×¡×™××•× ×™× ××™×•×ª×¨×™×
    text = re.sub(r'##\s*', '', text)
    text = re.sub(r'#+\s*', '', text)
    
    # × ×™×§×•×™ ×ª×’×™ HTML ×œ× × ×›×•× ×™×
    text = re.sub(r'<p>\s*</p>', '', text)  # ×ª×’×™ p ×¨×™×§×™×
    text = re.sub(r'<h\d>\s*</h\d>', '', text)  # ×ª×’×™ h ×¨×™×§×™×
    
    # ×”×¡×¨×ª ×©×•×¨×•×ª ×¨×™×§×•×ª ××™×•×ª×¨×•×ª
    text = re.sub(r'\n\s*\n\s*\n', '\n\n', text)
    
    # × ×™×§×•×™ ×¨×•×•×—×™× ××™×•×ª×¨×™× ×‘×ª×—×™×œ×ª ×•×‘×¡×•×£
    text = text.strip()
    
    return text

def convert_tagged_text_to_html(text):
    """
    ×”××¨×ª ×˜×§×¡×˜ ××¡×•××Ÿ (×›×•×œ×œ ×¡×™××•× ×™ markdown, ×›×•×ª×¨×•×ª ×•×¤×¡×§××•×ª ×‘×¢×‘×¨×™×ª) ×œ-HTML ×ª×§× ×™, ×ª×•×š ×”×¡×¨×” ××•×—×œ×˜×ª ×©×œ ×ª×•×•×™×•×ª ××™×•×ª×¨×•×ª.
    """
    if not text:
        return text
    
    lines = text.split('\n')
    processed_lines = []
    
    # ×“×¤×•×¡ ×œ×–×™×”×•×™ ×©×•×¨×•×ª ×ª×•×•×™×ª ××™×•×ª×¨×•×ª (×¢× ××• ×‘×œ×™ #)
    label_pattern = re.compile(r'^(#*)\s*(×¤×¡×§×”( ××©× ×”| ××—×¨×•× ×”)?(:|\s*:)?.*)$', re.UNICODE)
    # ×“×¤×•×¡ ×œ×–×™×”×•×™ "×›×•×ª×¨×ª ×¨××©×™×ª: ..."
    main_title_pattern = re.compile(r'^×›×•×ª×¨×ª ×¨××©×™×ª:\s*(.*)$')
    # ×“×¤×•×¡ ×œ×–×™×”×•×™ "×›×•×ª×¨×ª ××©× ×”: ..."
    subtitle_pattern = re.compile(r'^×›×•×ª×¨×ª ××©× ×”:\s*(.*)$')
    # ×“×¤×•×¡ ×œ×–×™×”×•×™ "×¤×¡×§×” ××©× ×”: ..."
    subpara_pattern = re.compile(r'^×¤×¡×§×” ××©× ×”:\s*(.*)$')
    # ×“×¤×•×¡ ×œ×–×™×”×•×™ "×¤×¡×§×” ××—×¨×•× ×”: ..."
    lastpara_pattern = re.compile(r'^×¤×¡×§×” ××—×¨×•× ×”:\s*(.*)$')
    # ×“×¤×•×¡ ×œ×–×™×”×•×™ TITLE# ...
    title_hash_pattern = re.compile(r'^TITLE#\s*(.*)$')
    # ×“×¤×•×¡ ×œ×–×™×”×•×™ SUBTITLE# (×¢× ××• ×‘×œ×™ ##)
    subtitle_hash_pattern = re.compile(r'^(##\s*)?SUBTITLE#\s*(.*)$')
    # ×“×¤×•×¡ ×œ×–×™×”×•×™ ## #pattern (like PFE uses)
    subtitle_hash_alt_pattern = re.compile(r'^##\s*#([^#]+)$')
    # ×“×¤×•×¡ ×œ×–×™×”×•×™ ## ## pattern (like INCY uses)
    subtitle_double_hash_pattern = re.compile(r'^##\s*##\s*(.*)$')
    # ×“×¤×•×¡ ×œ×–×™×”×•×™ #PARA# ...
    para_hash_pattern = re.compile(r'^#PARA#\s*(.*)$')
    # ×“×¤×•×¡ ×œ×–×™×”×•×™ ### PARA# ...
    para_hash_triple_pattern = re.compile(r'^###\s*PARA#\s*(.*)$')
    # ×“×¤×•×¡ ×œ×–×™×”×•×™ # ×¤×¡×§×” ×¨××©×•× ×”:, # ×¤×¡×§×” ×©× ×™×™×”: ×•×›×•'
    hebrew_para_pattern = re.compile(r'^#\s*×¤×¡×§×”\s+(×¨××©×•× ×”|×©× ×™×™×”|×©×œ×™×©×™×ª|×¨×‘×™×¢×™×ª|×—××™×©×™×ª|×©×™×©×™×ª|×©×‘×™×¢×™×ª|×©××™× ×™×ª|×ª×©×™×¢×™×ª|×¢×©×™×¨×™×ª):\s*(.*)$')

    for line in lines:
        line = line.strip()
        if not line:
            continue
        # ×“×œ×’ ×¢×œ ×©×•×¨×•×ª ×©×”×Ÿ ×¨×§ ×ª×•×•×™×ª ×¤×¡×§×” ("×¤×¡×§×” ×¨××©×•× ×”:", "×¤×¡×§×” ×©× ×™×™×”:" ×•×›×•')
        if label_pattern.match(line):
            continue
        # ×›×•×ª×¨×ª ×¨××©×™×ª
        m = main_title_pattern.match(line)
        if m:
            title = m.group(1).strip()
            if title:
                processed_lines.append(f'<h1>{title}</h1>')
            continue
        # ×›×•×ª×¨×ª ××©× ×”
        m = subtitle_pattern.match(line)
        if m:
            subtitle = m.group(1).strip()
            if subtitle:
                processed_lines.append(f'<h2>{subtitle}</h2>')
            continue
        # ×¤×¡×§×” ××©× ×”
        m = subpara_pattern.match(line)
        if m:
            subpara = m.group(1).strip()
            if subpara:
                processed_lines.append(f'<h3>{subpara}</h3>')
            continue
        # ×¤×¡×§×” ××—×¨×•× ×”
        m = lastpara_pattern.match(line)
        if m:
            lastpara = m.group(1).strip()
            if lastpara:
                processed_lines.append(f'<h3>{lastpara}</h3>')
            continue
        # TITLE# pattern
        m = title_hash_pattern.match(line)
        if m:
            title = m.group(1).strip()
            if title:
                processed_lines.append(f'<h1>{title}</h1>')
            continue
        # SUBTITLE# pattern
        m = subtitle_hash_pattern.match(line)
        if m:
            subtitle = m.group(2).strip()
            if subtitle:
                processed_lines.append(f'<h2>{subtitle}</h2>')
            continue
        # ## #pattern (like PFE uses)
        m = subtitle_hash_alt_pattern.match(line)
        if m:
            subtitle = m.group(1).strip()
            if subtitle:
                processed_lines.append(f'<h2>{subtitle}</h2>')
            continue
        # ## ## pattern (like INCY uses)
        m = subtitle_double_hash_pattern.match(line)
        if m:
            subtitle = m.group(1).strip()
            if subtitle:
                processed_lines.append(f'<h2>{subtitle}</h2>')
            continue
        # #PARA# pattern
        m = para_hash_pattern.match(line)
        if m:
            para_text = m.group(1).strip()
            if para_text:
                processed_lines.append(f'<p>{para_text}</p>')
            continue
        # ### PARA# pattern
        m = para_hash_triple_pattern.match(line)
        if m:
            para_text = m.group(1).strip()
            if para_text:
                processed_lines.append(f'<p>{para_text}</p>')
            continue
        # Hebrew paragraph pattern (# ×¤×¡×§×” ×¨××©×•× ×”:, etc.)
        m = hebrew_para_pattern.match(line)
        if m:
            para_text = m.group(2).strip()
            if para_text:
                processed_lines.append(f'<p>{para_text}</p>')
            continue
        # ### pattern for paragraphs (like ORCL uses)
        if line.startswith('### ') and not line.startswith('### PARA#') and not line.startswith('### SUBTITLE#'):
            para_text = line[4:].strip()
            if para_text:
                processed_lines.append(f'<p>{para_text}</p>')
            continue
        # markdown
        if line.startswith('### '):
            processed_lines.append(f'<h3>{line[4:]}</h3>')
            continue
        if line.startswith('## '):
            processed_lines.append(f'<h2>{line[3:]}</h2>')
            continue
        if line.startswith('# '):
            processed_lines.append(f'<h1>{line[2:]}</h1>')
            continue
        # ×›×œ ×©××¨ ×”×©×•×¨×•×ª - ×¤×¡×§×” ×¨×’×™×œ×”
        processed_lines.append(f'<p>{line}</p>')
    
    return '\n'.join(processed_lines)

# ×”×¤×¢×œ×ª ××•×“×œ Ollama ×¢× prompt ××¢×•×“×›×Ÿ
def process_with_gemma(original_text, ticker_info=None):
    """
    Process the original text with the LLM (aya-expanse:8b) using ONLY rephrasing and restructuring rules.
    Returns the processed text as a string with markers (#TITLE#, #SUBTITLE#, #PARA#).
    """
    prompt = generate_prompt(original_text, ticker_info)

    if ticker_info:
        prompt += "\n---\n××™×“×¢ × ×•×¡×£ ×¢×œ ×”×—×‘×¨×”:\n"
        for k, v in ticker_info.items():
            prompt += f"{k}: {v}\n"

    prompt += "\n---\n×”×—×–×¨ ×˜×§×¡×˜ ×‘×œ×‘×“, ×›×œ ×©×•×¨×” ××ª×—×™×œ×” ×‘××—×“ ××”×‘××™×: #TITLE#, #SUBTITLE#, #PARA#. ××™×Ÿ ×œ×”×—×–×™×¨ ×ª×’×™ HTML, markdown, JSON ××• ×ª×’×™× ××—×¨×™×."

    try:
        result = subprocess.run(
            ["ollama", "run", "aya-expanse:8b"],
            input=prompt.encode("utf-8"),
            capture_output=True
        )
        output = result.stdout.decode("utf-8").strip()
        logger.debug(f"ğŸ” DEBUG: Raw LLM output (first 200 chars): {output[:200]}...")
        
        # × ×™×§×•×™ ×”×¤×œ×˜ ××›×œ ×¡×•×’×™ JSON ×•×ª×’×™×
        cleaned_output = clean_llm_text(output)
        logger.debug(f"ğŸ” DEBUG: After clean_llm_text (first 200 chars): {cleaned_output[:200]}...")
        
        # ×”×¡×¨×ª ×ª×’×™× ×•×¢×•×‘×“×•×ª ×× ×¢×“×™×™×Ÿ ×§×™×™××™×
        cleaned_output = remove_json_artifacts(cleaned_output)
        logger.debug(f"ğŸ” DEBUG: After remove_json_artifacts (first 200 chars): {cleaned_output[:200]}...")
        
        # ×”×—×–×¨×ª ×”×˜×§×¡×˜ ×”×’×•×œ××™ ×¢× ×”×¡×™××•× ×™× - ×œ× HTML!
        return cleaned_output
        
    except Exception as e:
        logger.error(f"âŒ Error running ollama: {e}")
        return clean_llm_text("×©×’×™××” ×‘×¢×™×‘×•×“ LLM: " + str(e))

def remove_json_artifacts(text):
    """Remove JSON artifacts, tags, and facts from the text"""
    if not text:
        return text
    
    # ×”×¡×¨×ª JSON ××œ×
    text = re.sub(r'^\s*\{.*?"text":\s*"', '', text, flags=re.DOTALL)
    text = re.sub(r'",\s*"tags":\s*\[.*?\]\s*,\s*"facts":\s*\[.*?\]\s*\}\s*$', '', text, flags=re.DOTALL)
    text = re.sub(r'",\s*"tags":\s*\[.*?\]\s*\}\s*$', '', text, flags=re.DOTALL)
    text = re.sub(r'",\s*"facts":\s*\[.*?\]\s*\}\s*$', '', text, flags=re.DOTALL)
    text = re.sub(r'"\s*\}\s*$', '', text)
    
    # ×”×¡×¨×ª ×ª×’×™× ×•×¢×•×‘×“×•×ª ×‘×•×“×“×™×
    text = re.sub(r',\s*"tags":\s*\[.*?\]', '', text, flags=re.DOTALL)
    text = re.sub(r',\s*"facts":\s*\[.*?\]', '', text, flags=re.DOTALL)
    
    # ×”×¡×¨×ª markdown
    text = re.sub(r'```json\s*', '', text)
    text = re.sub(r'```\s*', '', text)
    
    # × ×™×§×•×™ × ×•×¡×£
    text = re.sub(r'^\s*"', '', text)
    text = re.sub(r'"\s*$', '', text)
    text = re.sub(r'\\n', '\n', text)
    text = re.sub(r'\\"', '"', text)
    
    return text.strip()

def clean_llm_text(text):
    """Clean LLM output from JSON artifacts, HTML tags, markdown symbols, and formatting issues"""
    if not text:
        return text
    
    # Remove JSON structure artifacts
    text = re.sub(r'^\s*\{\s*', '', text)
    text = re.sub(r'\s*\}\s*$', '', text)
    text = re.sub(r'^\s*"text":\s*"', '', text)
    text = re.sub(r'^\s*"":\s*"', '', text)
    text = re.sub(r'"\s*,\s*"tags":\s*\[\s*\]\s*$', '', text)
    text = re.sub(r'"\s*$', '', text)
    
    # Remove HTML tags
    text = re.sub(r'<[^>]+>', '', text)
    
    # Remove markdown symbols
    text = re.sub(r'^#+\s*', '', text)  # Remove markdown headers
    text = re.sub(r'\*\*([^*]+)\*\*', r'\1', text)  # Remove bold markdown
    text = re.sub(r'\*([^*]+)\*', r'\1', text)  # Remove italic markdown
    
    # Clean up newlines and whitespace
    text = re.sub(r'\\n', '\n', text)
    text = re.sub(r'\n\s*\n', '\n\n', text)  # Normalize paragraph breaks
    text = re.sub(r' +', ' ', text)  # Normalize spaces
    
    return text.strip() 